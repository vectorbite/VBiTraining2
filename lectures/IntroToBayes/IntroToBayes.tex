\documentclass[12pt,xcolor=svgnames]{beamer}
\usepackage{dsfont,natbib,setspace}
\usepackage{tabularx}
\usepackage{accents}
\mode<presentation>

% replaces beamer foot with simple page number
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{
  \vspace{-1.5cm}
  \raisebox{10pt}{\makebox[\paperwidth]{\hfill\makebox[20pt]{\color{gray}\scriptsize\insertpagenumber}}}}

% colors
\newcommand{\bk}{\color{black}}
\newcommand{\rd}{\color{red}}
\newcommand{\fg}{\color{ForestGreen}}
\newcommand{\mar}{\color{Maroon}}
\newcommand{\org}{\color{Orange}}
\newcommand{\bl}{\color{blue}}
\newcommand{\gr}{\color{gray}}
\newcommand{\theme}{\color{FireBrick}}
\newcommand{\fb}{\color{FireBrick}}
\newcommand{\hlf}{\setstretch{1}}

% common math markups
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mbox{\boldmath $#1$}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}

% spacing and style shorthan
\newcommand{\sk}{\vspace{.4cm}}
\newcommand{\nochap}{\vspace{0.5cm}}
\newcommand{\nsk}{\vspace{-.4cm}}
\newcommand{\chap}[1]{{\theme \Large \bf #1} \sk}
\newcommand{\R}[1]{{\bl\tt #1}}
\newcommand{\til}{{\footnotesize$\bs{\stackrel{\sim}{}}$ }}

% specific stats markups for this doc
\newcommand{\E}{\ds{E}}
\newcommand{\Reals}{\ds{R}}
\newcommand{\pr}{\text{Pr}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\mT}{\mc{T}}
\newcommand{\GP}{\mc{GP}}
\newcommand{\iidsim}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\indsim}{\stackrel{\mathrm{ind}}{\sim}}
\newcommand{\mN}{\mc{N}}
\newcommand{\mL}{\mc{L}}


\begin{document}

{ \usebackgroundtemplate{}%\includegraphics[height=\paperheight]{phoenix}}
\thispagestyle{empty}
\setcounter{page}{0}

%first review sampling distributions, then talk about sampling distributions for the OLS parameters, then prediction
%right now it goes prediction, review, OLS params, prediction

\title{\theme \Large \vskip 0.5cm
{\bf VectorBiTE Training 2018 \\ Methods Workshop}\\
\bigskip
\bf {\sf \gr Introduction to Bayesian Statistics}}

\author{
\begin{center}
\includegraphics[scale=0.15,trim=10 10 0 150]{VB_logo}
\end{center}
\texttt{\rd\small www.vectorbite.org}
%Normal distribution, distribution of the sample mean, \\
%Central Limit Theorem, Confidence Intervals
%and Method of Moments
%Review of sampling distributions
%Parameter estimation, sampling distributions,\\ 
%coefficient testing, and prediction intervals\\
%\vskip 1cm {\bf Leah R.~Johnson}\\ 
%{\org Virginia Tech}, Statistcs\\
%  \vskip .5cm \texttt{\rd\small leah.johnson-gramacy.com/QED/teaching}
}
\date{}
\maketitle 
}

% doc spacing
\setstretch{1.1}


\begin{frame}
\chap{Learning Objectives}

\begin{enumerate}
\item Understand the basic principles underlying Bayesian modeling methodology
\item  Introduce how to use Bayesian inference for real-world problems
\item Introduce computation tools to perform inference for simple models in R (how to turn the Bayesian crank)
\item Appreciate the need for sensitivity analysis, model checking and comparison, and the potential dangers of Bayesian methods.
\end{enumerate}

\end{frame}


\begin{frame}
\chap{Recall: Bayes Theorem} 

Bayes Theorem allows us to relate the conditional probabilities of two events $A$ and $B$:

\begin{equation*}
\pr(A|B) = \frac{\pr(B|A)\pr(A)}{\pr(B)}
\end{equation*}

\end{frame}

\begin{frame}
\chap{What is Bayesian Inference?} 

In the Bayesian approach our probabilities numerically represent rational beliefs. Bayes rule provides a rational method for updating those beliefs in light of new information and incorporating/quantifying uncertainty in those beliefs.\\

\sk

Thus, Bayesian inference is an approach for understanding data inductively. 

\end{frame}

\begin{frame}
\chap{What is Bayesian Inference?} 

We can re-write Bayes rule in terms of our parameters, $\theta$ and our data, $Y$:
\begin{align*}
\text{Pr}(\theta|Y) & = \frac{\text{Pr}(Y|\theta)\text{Pr}(\theta)}{\text{Pr}(Y)}\\
\end{align*}

The LHS is the main quantity of interest in a Bayesian analysis, the {\bl posterior},  denoted $f(\theta|Y)$:
\begin{equation*}
\overbrace{f(\theta|Y)}^\text{Posterior} \propto \overbrace{\mathcal{L}(\theta; Y)}^\text{Likelihood} \times \overbrace{f(\theta)}^\text{Prior}
\end{equation*}


\end{frame}


\begin{frame}
\chap{Bayesian methods provide} 

\begin{enumerate}
\item models for rational, quantitative learning
\item parameter estimates with good statistical properties
\item estimators that work for small and large sample sizes
\item parsimonious descriptions of data, predictions for missing data, and forecasts for future data
\item a coherent computational framework for model estimation, selection and validation
\end{enumerate}


\end{frame}



\begin{frame}
\chap{Classical vs Bayesian} 

The fundamental differences between classical and Bayesian methods is what is fixed and what is random in an analysis

\sk
\sk
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Paradigm & Fixed & Random \\
\hline
Classical & param ($\theta$) & data ($Y$)\\
\hline
Bayesian & data ($Y$) & param ($\theta$)\\
\hline
\end{tabular}
\end{center}
\end{frame}


\begin{frame}
\chap{Why/Why Not Bayesian Statistics?} 

{\footnotesize
{\bf \bl Pros}
\begin{enumerate}
\item If $f(\theta)$ \& $\mathcal{L}(\theta; Y)$ represent a rational person's beliefs, then Bayes' rule is an optimal method of updating these beliefs given new info (Cox 1946, 1961; Savage 1954; 1972).
\item Provides more intuitive answers in terms of the probability that parameters have particular values. 
\item In many complicated statistical problems there are no obvious non-Bayesian inference methods.
\end{enumerate}
{\bf \bl Cons}
\begin{enumerate}
\item It can be hard to mathematically formulate prior beliefs (choice of $f(\theta)$ often ad hoc or for computational reasons)
\item Posterior distributions can be sensitive to prior choice.
\item Analyses can be computationally costly.
\end{enumerate}
}

\end{frame}





\begin{frame}
\chap{Steps to Making Inference} 


\begin{enumerate}
\item Research question
\item Data collection
\item Model $Y_i \approx f(X_i)$ 
\item Estimate the parameter in the model with uncertainty
\item Make inference
\end{enumerate}

The difference between {\bl Classical} and {\bl Bayesian} lies in step 4: (C) uses maximum likelihood estimate (MLE), and (B) derives a posterior distribution.
\end{frame}


\begin{frame}
\chap{Example: Estimating the probability of a rare event}

Suppose we are interested in the prevalence of an infectious disease in a small city.  A small random sample of 20 individuals will be checked for infection.
\begin{itemize}
\item Interest is in the fraction of infected individuals
\begin{align*}
\theta \in \Theta =[0,1]
\end{align*}
\item The data records the number of infected individuals
\begin{align*}
y \in \mathcal{Y} =\{0,1, \ldots, 20\}
\end{align*}
\end{itemize}

\end{frame}

\begin{frame}
\chap{Example: Likelihood/sampling model}

Before the sample is obtained, the number of infected individuals is unknown. 
\begin{itemize}
\item Let $Y$ denote this to-be-determined value
\item If $\theta$ were known, a sensible {\bl sampling} model is
\begin{align*}
Y|\theta \sim \mr{Bin} (20, \theta)
\end{align*}
\end{itemize}
\begin{center}
\includegraphics[scale=0.375,trim=20 50 0 100]{bin_sampling}
\end{center}

\end{frame}


\begin{frame}
\chap{Example: Prior}

Other studies from various parts of the country indicate that the infection rate ranges from about
0.05 to 0.20, with an average prevalence of 0.1. 
\begin{itemize}
\item Moment matching from a beta distribution (a convenient choice) gives the prior
\begin{align*}
\theta \sim \mr{Beta} (2,20)
\end{align*}
\end{itemize}
\begin{center}
\includegraphics[scale=0.55,trim=10 50 0 125]{beta_prior}
\end{center}

\end{frame}


\begin{frame}
\chap{Example: Posterior}

The prior and sample model combination:
\begin{align*}
\theta & \sim \mr{Beta} (a,b)\\
Y|\theta &  \sim \mr{Bin} (n, \theta)
\end{align*}
and an observed $y$ (the data), leads to the posterior 
\begin{align*}
p(\theta|y)=\mr{Beta}(a+y, b+n-y)
\end{align*}

\end{frame}


\begin{frame}
\chap{Example: Posterior}

For our case, we have $a=2$, $b=20$, $n=20$. \\

If we don't find any infections, then $y=0$ our posterior is:
\begin{align*}
p(\theta|y=0)=\mr{Beta}(2, 40)
\end{align*}
\begin{center}
\includegraphics[scale=0.55,trim=10 50 0 60]{beta_bin_posterior}
\end{center}

\end{frame}

\begin{frame}
\chap{Example: Sensitivity Analysis }

How influential is our prior?

The posterior expectation is
\begin{align*}
\mr{E}\{\theta|Y=y\} =   \frac{n}{w+n} \bar{y} + \frac{w}{w+n} \theta_0
\end{align*}
a weighted average of the sample mean and the prior expectation:
\begin{align*}
\theta_0 & =  \frac{a}{a+b} ~~~~ {\rd \rightarrow} \text{ prior expectation (or guess)}\\
w & = a + b  ~~~~ {\rd \rightarrow} \text{  prior confidence} 
\end{align*}


\end{frame}


\begin{frame}
\chap{Example: A non-Bayesian approach}

A standard estimate of a population proportion, $\theta$ is the sample mean  $\bar{y} = y/n$, the fraction of infected people in the sample. 

\sk
If $y=0$, this gives zero, so reporting the sampling uncertainty is crucial (e.g., for reporting to health officials). 

\sk
The most popular 95\% confidence interval for a population proportion is the {\bl Wald Interval}:
\begin{equation*}
\bar{y} \pm 1.96 \sqrt{\bar{y}(1-\bar{y})/n}.
\end{equation*}
This has the correct {\it asymptotic} coverage, but {\rd $y=0$ is still problematic}!

\end{frame}


\begin{frame}
\nochap
{\bf \fb Exercise: is a treatment for cancer effective?}\\

{\footnotesize
We have data on $n$ cancer patients that have been given a particular treatment. Our outcome variable is whether or not it was ``effective'':
\begin{center}
 \begin{tabular}{lll}
Patient & Effectiveness & Numerical Data \\
1& N& 0\\
2&N&0\\
3&Y&1\\
\vdots &\vdots &\vdots \\
\end{tabular}
\end{center}
The appropriate sampling model for each patient is a Bernoilli:
$$Y_i|\theta  \stackrel{iid}{\sim} f(Y|\theta) = \text{Bern}(\theta)$$ 
where $\theta$ is the success rate of the treatment. Based on this, write down the likelihood for the $n$ patients. Then, assuming a $\mr{Beta}(a,b)$ prior for $\theta$ find the posterior distribution for $\theta|Y$. Does this look familiar?
}

%{\bf ALTERNATIVE}: Midge normal model

\end{frame}



\begin{frame}
\chap{Conjugate Bayesian Models}

Some sets of priors/likelihoods/posteriors exhibit a special relationship called {\em \bl conjugacy}. This happens if the posterior distribution has the same form as the prior. For instance, in our above Beta-Binomial/Bernoilli examples:
\begin{align*}
\theta & \sim \mr{Beta} (a,b)\\
Y|\theta &  \sim \mr{Bin} (n, \theta)\\
\theta | Y & \sim \mr{Beta}(a^*, b^*)
\end{align*}
{\em Are all posteriors in the same family as the priors?} {\rd \bf No}\\

Conjugacy is a nice special property, but most of the time this isn't the case and getting an analytic form of the posterior distribution can be hard or impossible.  

\end{frame}

\begin{frame}
\chap{What do you do with a Posterior?}

\begin{itemize}
\item Summarize important aspects of the posterior
	\begin{itemize}
	\item mean, median, mode, variance...
	\end{itemize}
\item Check sensitivity of posterior to prior choice
\item Say what range of parameters is consistent with the observed data given our prior information
\item Make predictions
\end{itemize}

\end{frame}

\begin{frame}
\chap{Posterior Summaries}

If we are interested in point-summaries of our posterior then the (full) distribution allows us many choices. For example for the Beta-Binomial model, we found that
\begin{equation*}
p(\theta | y)=\mr{Beta}(a+y, b+n-y)
\end{equation*}
So, for example:
\begin{align*}
\mr{E}[\theta|Y] & = \frac{a+y}{a+b+n} \\
\mr{mode}(\theta|Y) & = \frac{a+y-1}{a+b+n-2} ~~~ \dagger \\
\end{align*}
\vspace{0.25cm}
\hfill {\bl \footnotesize $\dagger$ a.k.a. the maximum {\em a posteriori estimator} (MAP)}
\end{frame}


\begin{frame}
\chap{Prior Sensitivity}

As we saw earlier for the Beta-Binomial model, the posterior expectation can be written as a weighted average of information from the prior and the data
\begin{align*}
\mr{E}\{\theta|Y=y\} =   \frac{n}{a + b +n} \bar{y} + \frac{a+b}{a+b+n} \theta_0.
\end{align*}
Thus $a$ and $b$ can be interpreted here as ``prior data'' where $a$ is the number of ``prior successes'' and $a+b$ is the ``prior sample size''. When $n\gg a+b$ most of our information comes from the data instead of the prior. 
\end{frame}

\begin{frame}
\chap{Prior Sensitivity}

We usually visualize the prior vs.~posterior to check for sensitivity, since we don't have analytic ways, in general. 
\begin{center}
\includegraphics[scale=0.5,trim=30 50 0 20]{prior_sens}
\end{center}
\end{frame}


\begin{frame}
\chap{Confidence Regions}

An interval $[l(y), u(y)]$, based on the observed data $Y=y$, has 95\% Bayesian coverage for $\theta$ if
\begin{align*}
P(l(y) <\theta < u(y)|Y=y)=0.95
\end{align*}
The interpretation: it describes your information about the true value of $\theta$ after you have observed $Y=y$.\\

\sk
Such intervals are typically called {\bl credible intervals}, to distinguish them from frequentist confidence intervals. Both are referred to as CIs. 

\end{frame}

\begin{frame}
\chap{Quantile-based (Bayesian) CI}

Perhaps the easiest way to obtain a credible interval is to use the posterior quantiles.

To make a $100 \times (1-\alpha)$\% quantile-based CI, find numbers $\theta_{\alpha/2}<\theta_{1- \alpha/2}$ such that
\begin{enumerate}
\item $P(\theta <\theta_{\alpha/2} |Y=y)=\alpha/2$
\item $P(\theta >\theta_{1-\alpha/2} |Y=y)=\alpha/2$
\end{enumerate}
The numbers $\theta_{\alpha/2},\theta_{1- \alpha/2}$ are the $\alpha/2$ and  $1-\alpha/2$ posterior quantiles of $\theta$.\\

\end{frame}


\begin{frame}[fragile]
\chap{Example: Binomial sampling + uniform prior}

Suppose out of $n=10$ conditionally independent draws of a binary random variable we observe $Y=2$ ones (successes). \\

\sk
Using a uniform prior distribution (a.k.a., $\mr{Beta}(1,1)$) for $\theta$, the posterior distribution is $\theta | Y=2 ~ \mr{Beta}(1+2,1+10-2)$. \\

\sk
A 95\% CI can be obtained from the 0.025 and 0.975 quantiles of this beta distribution (e.g., using {\sf R}):\\
{\bl \footnotesize
\begin{verbatim}
> round(qbeta(p=c(0.025, 0.975), 3, 9), 2)
[1] 0.06 0.52
\end{verbatim}
}
Thus the posterior probability that $\theta \in [0.06, 0.52]$ is 95\%. 
\end{frame}


\begin{frame}[fragile]
\chap{Example: Binomial sampling + uniform prior}

One drawback: Notice that there are $\theta$-values outside the quantile-based CI that have higher probability [density] than some points inside the interval.

\begin{center}
\includegraphics[scale=0.5,trim=30 50 0 50]{quantCI}
\end{center}

\end{frame}


\begin{frame}
\chap{HPD region}

An alternative is a more restrictive type of interval. A $100 \times(1-\alpha)$\% highest posterior density (HPD) regions is the part of parameter space, $s(y)$, such that:
\begin{enumerate}
\item $P(\theta \in s(y) |Y=y)= 1-\alpha$
\item If $\theta_a \in s(y)$ and $\theta_b \notin s(y)$ then $P(\theta_a |Y=y)>P(\theta_b |Y=y)$
\end{enumerate}
So that all points in the HPD region have higher probability density than those outside.

\end{frame}


\begin{frame}
\chap{Example: Binomial sampling + uniform prior}

Numerically, we collect the highest density points with cumulative density greater that $1-\alpha$: 
\begin{center}
\includegraphics[scale=0.275,trim=30 30 0 20]{HPD_example}
\end{center}
The 95\% HPD region is [0.04, 0.48] which is narrower than the quantile-based CI, yet both contain 95\% probability.
\end{frame}


\begin{frame}
\chap{Numerical Methods}

Most of the time we can't get a nice analytic form for a posterior distribution. If we go back to the full Bayes theorem:
\begin{align*}
\text{Pr}(\theta|Y) & = \frac{\mathcal{L}(\theta; Y)f(\theta)}{{\rd \text{Pr}(Y)}}
\end{align*}
We are usually specifying the likelihood and the prior but we often don't know the normalizing constant in the denominator. Without this, the probabilities don't properly integrate to 1 and we {\rd can't make probability statements}. We need a way to approximate the distribution \\
\end{frame}

\begin{frame}
\chap{Monte Carlo Simulation}

Instead we ``Monte Carlo'' (MC) methods to generate random deviates in the right ratios from the target posterior called ``{\bl draws}'' or samples. We then use these draws to approximate our distribution and make inference statements (estimates, CIs, etc). 

\sk
We can also use the draws to calculate the posterior distribution of {\bf \bl any function of our estimated parameters}. As the number of draws/samples gets large we can approximate these quantities arbitrarily high precision.


\end{frame}


\begin{frame}
\chap{Markov Chain MC (MCMC)}

The most commonly used numerical algorithm for generating posterior samples is MCMC. \\

\sk
A {\bl Markov Chain} is a randomly generated sequence of numbers where each draw depends on the one immediately preceding it $\rightarrow$ random walk.


\begin{center}
\hspace{2.75cm} \includegraphics[scale=0.375,trim=30 80 0 70]{MCMC}
\end{center}

\vfill
\hfill {\tiny Plot -- Ian Murray (http://mlg.eng.cam.ac.uk/zoubin/tut06/mcmc.pdf)} 
\end{frame}

\begin{frame}
\chap{Gibbs Sampling}

One specific algorithm that is commonly used is {\bl Gibbs Sampling}. \\

\sk
Gibbs sampling leverages the {\em conditional} distributions of parameters to generate samples by proposing them one at a time. This is the algorithm implemented in the popular Bayesian packages {\sf BUGS}, {\sf WinBUGS}, and {\sf JAGS}/{\tt rjags}. 

\sk 
We will treat Gibbs sampling and other of the numerical methods as mostly ``black boxes''. We'll learn to diagnose output from these later on in the practical component.


\end{frame}


\begin{frame}
\chap{What do we do with Posterior Samples?}

We can treat the draws much like we would data:
\begin{itemize}
\item Calculate posterior summaries (mean, median, mode, etc) just like we would a data sample
\item Calculate precision of the summaries (e.g., sample variance)
\item CIs via quantiles (order statistics of the data) or HPD intervals (using {\tt CODA} package in R)
\end{itemize}

If the samples are parameters in a complex model, we can plug them all in, one at a time to get a range of possible predictions from the model (we'll see this in the practical bit, later on). 

\end{frame}

\begin{frame}
\chap{How do we compare models?}

The simplest way that we will use to compare models is via the {\bl Deviance Information Criterion} (DIC). Like AIC and BIC, DIC seeks to judge a model on how well it fits, penalized by the complexity of the model.
\begin{align*}
DIC = D(\bar{\theta}) + 2p_D
\end{align*}
where:
\begin{itemize}
\item Deviance: $D(\theta)=-2\log(\mathcal{L}(\theta; y)) + C$
\item Penalty: $p_D = \bar{D} -D(\bar{\theta})$
\item $D(\bar{\theta})$: deviance at the posterior mean of $\theta$
\item $\bar{D}$: average deviance across the posterior samples.
\end{itemize}
{\rd $\rightarrow$ Already implemented in JAGS!}
\end{frame}


\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{frame}
\begin{enumerate}



\item[4. ] {\bf Posterior:}
First we need to decide on an appropriate  {\bl prior} for $\rho.$ To look at the domain of $\rho$ we recall that 
\begin{equation*}
\rho = \pr(Y_i = 1),\Rightarrow \rho \in [0, 1] 
\end{equation*}
Hence, either a {\bl Beta} or a {\bl Uniform} distribution can be a prior for $\rho.$ Say we chose $\text{Beta}(a,b)$ with $a$ and $b$ specified as $a=b=1$ or $a=1,b=3$. Thus,

$$ f(\rho) = \text{Beta}(a,b).$$
\end{enumerate}
\end{frame}

\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

Next, we need to derive the likelihood. We have $Y_i|\rho   \stackrel{iid}{\sim} f(Y|\rho) = \text{Bern}(\rho),$ where $Y_i|\rho$ are assumed to be {\bl conditionally} independent and identically distributed, thus the likelihood is given by

\begin{align*}
f(\tilde{Y}|\rho) &= f(Y_1, Y_2, \cdots Y_5|\rho)\\
& \stackrel{iid}{=}  f(Y_1|\rho) f(Y_2|\rho)  \cdots f(Y_5|\rho)\\
& = \prod_{i=1}^{n=5} f(Y_i|\rho).
\end{align*}
\end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

Now we can use {\bl Bayes} formula to calculate the posterior distribution as follows
\begin{align*}
f(\rho|\tilde{Y}) &\propto \prod_{i=1}^{n=5} f(Y_i|\rho) f(\rho)\\
&\propto  \prod_{i=1}^{n=5} \text{Bern}(Y_i|\rho)\text{Beta}(a,b)\\
&\propto  \prod_{i=1}^{n=5} \rho^{Y_i} (1- \rho)^{n-Y_i} \dfrac{\Gamma (a+b)}{\Gamma (a)\Gamma (b)} \rho^{a-1} (1-\rho)^{b-1}
\end{align*}

Does this look like a known distribution? What do we do?

\end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

We play the game {\bl name that distribution}
\begin{enumerate}
\item[a.] Simplify the best you can
\item[b.] $\ast$ , $\div$ by constants
\item[c.] Remember what is the RV in the expression
\item[d.] Look for {\bl Kernels} of known distributions
\end{enumerate}
\sk
{\rd Definition:} The {\bl Kernel} of a probability density function (or mass function) are all components which include the random variable. 

eg: Let $Y_i \sim \textbf{N}(\mu, \sigma^2),$ with random variable $\mu$

$$ f(Y_i) = \underbrace{\dfrac{1}{\sqrt{2\pi \sigma^2}}}_{constant} \underbrace{e^{-\dfrac{1}{2\sigma^2}(Y_i - \mu)^2}}_{Kernel}$$ 
\end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[4. ] {\bf Posterior: continued} 

Back to our example: 
\begin{align*}
f(\rho|\tilde{Y}) &\propto  \prod_{i=1}^{n=5} \rho^{Y_i} (1- \rho)^{n-Y_i} \dfrac{\Gamma (a+b)}{\Gamma (a)\Gamma (b)} \rho^{a-1} (1-\rho)^{b-1}\\
&\propto \rho^{\sum Y_i} (1- \rho)^{n-\sum Y_i} \dfrac{\Gamma (a+b)}{\Gamma (a)\Gamma (b)} \rho^{a-1} (1-\rho)^{b-1}\\
&\propto \rho^{\sum Y_i +a -1} (1- \rho)^{n-\sum Y_i +b -1} \\
&\propto \textbf{Beta} \left(\underbrace{\sum Y_i +a}_{a^*}, \underbrace{n-\sum Y_i +b}_{b^*}\right),
\end{align*}
where
$ \textbf{Beta}(a^*, b^*) = \dfrac{\Gamma (a^*+b^*)}{\Gamma (a^*)\Gamma (b^*)} \rho^{a^*-1} (1-\rho)^{b^*-1}.$

{\rd Now what do we do with this posterior?}

\end{enumerate}
\end{frame}





\begin{frame}
\begin{enumerate}

\item[5.] {\bf Make inference:} We summarize the posterior to make inference using the following statistics
\begin{enumerate}
\item[a.] Expectation $\text{E}(\rho|\tilde{Y})$% = \dfrac{a^*}{a^* +b^*} = 0.25$
\item[b.] Maximum A Posteriori (MAP) $\text(Mode) (f(\rho|\tilde{Y})$% = = \dfrac{a^* -1}{a^* +b^* -2} = 0.2$
\item[c.] Variance $\text{Var}(\rho|\tilde{Y} )$%= (.12)^2$
\item[d.] Calculate $\pr(\rho<.5|\tilde{Y}) $%= .96$
\item[e.] Q Credible Interval $\pr(\rho \in [L, U] |\tilde{Y}) = Q,$ with $L = \dfrac{1-Q}{2},$ and $U = \dfrac{1-Q}{2} +Q$
\item[f.] Q Highest Posterior Density Interval (HPDI) $\pr(\rho \in [L, U] |\tilde{Y}) = Q,$ where $L$ and $U$ are chosen so that $U-L$ is the shortest
\end{enumerate}
 \end{enumerate}
\end{frame}


\begin{frame}
\begin{enumerate}

\item[5.] {\bf Inference Statements} 

{\bf For a, b, c:} Given the data, we predict that the probability that the cancer drug is effective is $\text{E}(\rho|\tilde{Y})$ or  $\text{MAP}(\rho|\tilde{Y})$ with variance $\text{Var}(\rho|\tilde{Y})$. \\

{\bf For d:} Given the data, we infer the prob of the cancer drug being effective is low because $\pr(\rho<.5|\tilde{Y}) = .96$ or high because $\pr(\rho<.5|\tilde{Y}) = .2$\\

{\bf For e, f:} Given the data, the probability that $\rho \in [L, U]$ is Q. \\

{\rd \bf Note:} {\bf QCI} Describes the behavior of the {\bl  tail} of the posterior distribution and { \bf HPDI} Describes where the {\bl mean} of the posterior distribution is.\\


\end{enumerate}
\end{frame}

\fi

