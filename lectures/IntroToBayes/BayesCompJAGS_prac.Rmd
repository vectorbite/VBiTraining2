---
title: "Example: Exact vs.~Numeric Bayesian analysis"
author: "VectorBiTE Training Workshop"
date: "June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Example: Midge Wing Length

We will use this simple example to go through the steps of assessing a Bayesian model and we'll see that MCMC can allow us to approximate the posterior distribution.

Grogan and Wirth (1981) provide data on the wing length (in millimeters) of nine members of a species of midge (small, two-winged flies). 

From these measurements we wish to make inference about the population mean $\mu$.


```{r}
# Load data
WL.data <- read.csv("../../data/MidgeWingLength.csv")
Y <- WL.data$WingLength
n <- length(Y)

hist(Y,breaks=10,xlab="Wing Length (mm)") 
```

## Non-Bayesian analysis

We might expect that these midge data could be draws from a _Normal_ distribution $\mathcal{N}(\mu, \sigma^2)$. Recall that the MLEs for $\mu$ and $\sigma^2$ here are simply the _sample mean_ and _sample variance_ respectively:
```{r}
m<-sum(Y)/n
s2<-sum((Y-m)^2)/(n-1)
round(c(m, s2), 3)
```

```{r}
x<-seq(1.4,2.2, length=50)
hist(Y,breaks=10,xlab="Wing Length (mm)", xlim=c(1.4, 2.2), freq=FALSE) 
lines(x, dnorm(x, mean=m, sd=sqrt(s2)), col=2)
```

__NOTE:__ I've plotted the estimate of the _population_ distribution here, but this is not the ___predictive distribution___ (which would be a Student T because we're estimating both the mean and variance...).

-----

The non-Bayesian version here has the advantage of being quick and familiar. However, from our point of view it has two weaknesses:

1. Because we have so few data points estimates of the accuracy of our predictions aren't available. 9 points is only barely enough to estimate a mean, so we don't trust any of the variance calculations.

2. We can't easily incorporate things that we might already know about midges into our analysis. 

Let's see how we can do a similar analysis using a Bayesian approach, first analytically, and the with JAGS.

## Setting up the Bayesian Model

We need to define the likelihood and the priors for our Bayesian analysis. Given the analysis that we've just done, let's assume that our data come from a normal distribution with unknown mean, $\mu$ but that we know the variance is $\sigma^2 = 0.025$. That is:
$$
\mathbf{Y} \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(\mu, 0.025^2)
$$


## Prior Information

Studies from other populations suggest that wing lengths are usually around 1.9 mm, so we set $\mu_0 = 1.9$

We also know that lengths must be positive ($\mu >0$)

We can approximate this restriction with a normal prior distribution for $\mu$ as follows:

Since most of the normal density is within two standard deviations of the mean we choose $\tau^2_0$ so that

$$ \mu_0 - 2\sigma_0 >0 \Rightarrow \sigma_0 <1.9/2 = 0.95 $$
I will choose $\sigma_0=0.8$ here. Thus our prior for mu will be:
$$
\mu \sim \mathcal{N}(1.9, 0.8^2)
$$

----

Together, then, our full model is:
$$
\begin{align*}
\mathbf{Y} & \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(\mu, 0.025^2)\\
\mu &\sim \mathcal{N}(1.9, 0.8^2)
\end{align*}
$$

## Analytic Posterior

For this very simple case it is easy to write down the posterior distribution (up to some constant). First, note that the likehood for the data can be written as 

$$
\begin{align*}
\mathcal{L} &\propto \prod_{i=1}^n \frac{1}{\sigma} \exp\left(-\frac{1}{2\sigma^2}(Y_i-\mu)^2 \right) \\
& =  \frac{1}{\sigma^n} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (Y_i-\mu)^2 \right)\\
& \propto \exp\left(-\frac{n}{2\sigma^2} (\bar{Y}-\mu)^2 \right)
\end{align*}
$$

Multiplying the prior through we get the following for the posterior:

$$
\mathrm{P}(\mu|\mathbf{Y}) \propto \exp \left(-\frac{n}{2\sigma^2} (\bar{Y}-\mu)^2 \right) \exp\left(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2 \right)
$$

You can re-arrange, complete the square, etc, to get a new expression that is like

$$
\mathrm{P}(\mu|\mathbf{Y}) \propto \exp \left(-\frac{1}{2\sigma_p^2} (\mu_p-\mu)^2 \right)
$$

where 

$$
\begin{align*}
\mu_p & = \frac{n\sigma_0^2}{\sigma^2 + n\sigma_0^2} \bar{Y} +  \frac{\sigma^2}{\frac{\sigma^2}{n} + \sigma_0^2} \mu_0\\
& \\
\sigma_p^2 & = \left( \frac{n}{\sigma^2} + \frac{1}{\sigma_0^2} \right)^{-1}
\end{align*}
$$

Instead of writing this last in terms of the variances, we could instead use precision (the inverse variance) which gives a simpler expression:
$$
\tau_p = n\tau + \tau_0
$$

Just like in our earlier example, our estimate of the mean is a weighted average of the data and the prior, with the variance being determined by the data and prior variances.

So lets write a little function to calculate $\mu_p$ and $\tau_p$ and the plug in our numbers

```{r}
tau.post<-function(tau, tau0, n){n*tau + tau0}
mu.post<-function(Ybar, mu0, sig20, sig2, n){
  weight<-sig2+n*sig20
  
  return(n*sig20*Ybar/weight + sig2*mu0/weight)
}
```

Let's plot 3 things together -- the data histogram, the prior, and the posterior

```{r}
mu0 <- 1.9
s20 <- 0.8
s2<- 0.025 ## "true" variance

mp<-mu.post(Ybar=m, mu0=mu0, sig20=s20, sig2=s2, n=n)
tp<-tau.post(tau=1/s2, tau0=1/s20, n=n)
```

```{r}
x<-seq(1.3,2.3, length=1000)
hist(Y,breaks=10,xlab="Wing Length (mm)", xlim=c(1.3, 2.3),
     freq=FALSE, ylim=c(0,8)) 
lines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior
lines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior
legend("topleft", legend=c("prior", "posterior"), col=c(2,4), lty=c(2,1), lwd=2)
```

## Practice: Prior sensitivity

Change the values of the mean and the variance that you choose for the prior ("hyperparameters"). What does this do to the posterior distribution. E.g., what happens if the variance you choose is small, and $\mu_0 =2.5$ or so. Is this what you expect?


## Numerical evaluation of the posterior with JAGS

Let's show that we can get the same thing from JAGS that we were able to get from the analytic results. You'll need to make sure you have installed JAGS (which must be done outside of R) and then the libraries ${\tt rjags}$ and ${\tt coda}$.

```{r, results="hide", warning=FALSE, message=FALSE}
# Load libraries
require(rjags) # does the fitting
require(coda) # makes diagnostic plots
##require(mcmcplots) # another option for diagnostic plots
```

## Specifying the model

First we must encode our choices for our data model and priors to pass them to the fitting routines in JAGS. This involves setting up a ${\tt model}$ that includes the likelihood for each data point and a prior for every parameter we want to estimate. Here is an example of how we would do this for the simple model we fit for the midge data (note that JAGS uses the precision instead of the variance or sd for the normal distribution_:

```{r}
model1 <- "model{

  ## Likelihood
  for(i in 1:n){
    Y[i] ~ dnorm(mu,tau)
  }

  ## Prior for mu
  mu        ~ dnorm(mu0,tau0)

} ## close model
"
```

Now we will create the JAGS model

```{r}
model <- jags.model(textConnection(model1), 
                    n.chains = 1, ## usually do more
                    data = list(Y=Y,n=n, ## data
                                mu0=mu0, tau0=1/s20, ## hyperparams
                                tau = 1/s2 ## known precision
                                ),
                    inits=list(mu=3) ## setting an starting val
                    )
```

Now we'll run the MCMC and, see how the output looks for a short chain with no burnin:


```{r}

samp <- coda.samples(model, 
        variable.names=c("mu"), 
        n.iter=1000, progress.bar="none")

```

```{r}
plot(samp)
```


MCMC is a rejection algorithm that often needs to converge or ``burn-in'' -- that is we need to potentially move until we're taking draws from the correct distribution. Unlike for optimization problems, this does not mean  that the algorithm heads toward a single value. Instead we're looking for a pattern where the draws are seemingly unrelated and random. To assess convergence we look at trace plots, the goal is to get traces that look like ``fuzzy caterpillars''. 

Sometimes at the beginning of a run, if we start far from the area near the posterior mean of the parameter, we will instead get something that looks like a trending time series. If this is the case we have to drop the samples that were taken during the burn-in phase. Here's an example of how to do that.


```{r}
update(model, 10000, progress.bar="none") # Burnin for 10000 samples

samp <- coda.samples(model, 
        variable.names=c("mu"), 
        n.iter=20000, progress.bar="none")

```


```{r}
plot(samp)
```

This is a very fuzzy caterpillar!

We can also use the summary function to examine the samples generated:
```{r}
summary(samp)
```

Let's compare these draws to what we got with our analytic solution:


```{r}
x<-seq(1.3,2.3, length=1000)
hist(samp[[1]], xlab="mu", xlim=c(1.3, 2.3),
     freq=FALSE, ylim=c(0,8), main ="posterior samples") 
lines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior
lines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior
legend("topleft", legend=c("prior", "analytic posterior"), col=c(2,4), lty=c(2,1), lwd=2)
```

It worked! 


As with the analytic approach, it's always a good idea when you run your analyses to see how sensitive is your result to the priors you choose. Unless you are purposefully choosing an informative prior, we usually want the prior and posterior to look different.


## Estimating the population variance
 
One advantage of the numerical approach is that we can choose almost anything we want for the priors on multiple parameters without worrying if they are conjugate, or if we want to include additional information. For example, let's say that, not, we want to force the mean to be positive (and also the data, perhaps), and concurrently estimate the variance. Here is a possible model.

```{r}
model2 <- "model{

  # Likelihood
  for(i in 1:n){
    Y[i] ~ dnorm(mu,tau) T(0,) ## truncates at 0
  }

  # Prior for mu
  mu    ~ dnorm(mu0,tau0)

  # Prior for the precision
  tau   ~ dgamma(a, b)

  # Compute the variance
  s2       <- 1/tau
}"
```

```{r}
## hyperparams for tau
a   <- 0.01
b   <- 0.01

m2 <- jags.model(textConnection(model2), 
                    n.chains = 1,
                    data = list(Y=Y, n=n,
                               mu0=mu0, tau0=1/s20, ## mu hyperparams
                                a=a, b=b ## tau hyperparams
                                ),
                    inits=list(mu=3, tau=10) ## starting vals
                    )
```


```{r}

samp <- coda.samples(m2, 
        variable.names=c("mu","s2"), 
        n.iter=1000, progress.bar="none")

```

```{r}
plot(samp)
```

```{r}
summary(samp)
```

Now we plot each with their priors:

```{r}
par(mfrow=c(1,2), bty="n")

hist(samp[[1]][,1], xlab="samples of mu", main="mu")
lines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), 
      col=2, lty=2, lwd=2) ## prior

x2<-seq(0, 200, length=1000)
hist(1/samp[[1]][,2], xlab="samples of tau", main="tau")
lines(x2, dgamma(x2, shape = a, rate = b), 
      col=2, lty=2, lwd=2) ## prior

```

We also want to look at the joint distribution of $\mu$ and $\sigma^2$:

```{r}
plot(as.numeric(samp[[1]][,1]), samp[[1]][,2], 
     xlab="mu", ylab="s2")
```

## Practice: Updating the model

Redo the previous analysis placing a gamma prior on $\mu$ as well. Set the prior so that the mean and variance are the same as in the normal example from above (use moment matching). Do you get something similar?






