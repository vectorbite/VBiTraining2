---
title: "VectorBiTE Training Workshop 2018 - Time Series Practice"
author: "Your Name"
date: "June 2018"
output:
  html_document: default
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
graphics: yes
---

```{r setup, include=FALSE}
set.seed(123)
knitr::opts_chunk$set(echo = TRUE)
```

### Overview and Instructions

The goal of this lab is to walk you through a simple analysis of the data on airline passenger data using our simple time series extensions to regression.  

Below you will add code to generate data, plots, etc., into the appropriate portions of the RMarkdown document. Be sure the answer all of the ___questions___. 


### The data

The data you will explore consist of data on monthly numbers of international airline passengers from 1949 to 1961. These data can be found in the ${\tt airline.csv}$ data file on GitHub. 

```{r}
airline <- read.csv("../../data/airline.csv")
```


First we plot the data. Notice I plot this a bit differently than the default. 

```{r, fig.align="center", fig.width=6, fig.height=4}
plot(airline$Passengers, xlab="year", ylab="monthly passengers",
     type="l", col=3, lwd=2, xaxt="n")
axis(1, at=(0:12)*12, labels=1949:1961)
```

___Q1: What does xaxt="n" mean? What does the axis function do?___


<br>
<Br>


Next, use the acf function to plot the auto-correlation function of the Passesngers data.
```{r, fig.align="center", fig.width=6, fig.height=4}
## put the acf function here


```

___Q2: From the two plots above, what things do you notice about the data? What transforms might you need to take of the data? What kinds of covariates might you need to add in?___


<br>
<br>


Re-plot the from data above using a log transform of the response (passenger).

```{r, fig.align="center", fig.width=6, fig.height=4}
## plot here

```


Now it's time to build a data frame to hold the data. This is a good habit to get in to when you are building models for data that include transforms and possibly multiple lags, etc. 

First we make a time covariate 
```{r}
t <- 2:nrow(airline)
```

Now, into the data frame, I'll add the following covariates (I've done this for you):

1. logY: log of the number of passengers 
2. logYpast: this is your auto-regressive term, the log of the passengers from the previous month
3. t: month number
4. sin12: sine terms with period of 12 months
5. cos12: cosine term with period of 12 months

```{r}
YX <- data.frame(logY=log(airline$Passengers[2:144]),
                 logYpast=log(airline$Passengers[1:143]), t=t,
                 sin12=sin(2*pi*t/12), cos12=cos(2*pi*t/12))
```

Fit a linear model with logY as the response and the other 4 components as predictors. Look at the summary of the fit.

```{r}
## your fitted model and the summary go here

```

___Q3: Are all of the predictors significant? What is the $R^2$ of your regression?___


<br>
<br>

Next, we want to plot the data along with the prediction (aka the fit). I've plotted the data on a log scale (drawn with a dotted line).  Use the "lines" function to overlay the FITTED values from your regression (e.g., if your regression model was called "reg" you want to plot reg$fitted vs t) as a solid line in another color. This solid line is your prediction. Update the legend to reflect your additional line.
```{r, fig.align="center", fig.width=6, fig.height=4}
plot(log(airline$Passengers), xlab="year",
     ylab="log monthly passengers", type="l", col=4, lty=2,
     xaxt="n", lwd=2)
axis(1, at=(0:12)*12, labels=1949:1961)
## add in the line here


legend("topleft", legend=c("data", "fitted"), lty=c(2,1), col=c(4,4)) ## update the legend
```

The difference between the solid and dotted lines at each month are your residuals across time. As always, we want to also look at our residuals explicitly to see if we're doing a good job of explaining things. For TS we primarily look at residuals across time, and the ACF of our residuals. So make those two plots here. 

```{r, fig.align="center", fig.width=6, fig.height=4}
par(mfrow=c(1,2))
## residuals plotted across time

## acf plot of the residuals

```


__Q4: How do these look? What do you notice about the residuals, esp the ACF?___


<br>
<br>


It turns out that there is a month effect that we're missing. Here
is one way to look at it (note we have to index by t so that everything lines up properly):

```{r, fig.align="center", fig.width=6, fig.height=4}
## this command assumes that your fitted model is called mod1. You'll need to change it to your object

##boxplot(mod1$resid ~ airline$Month[t], xlab="month",
##        ylab="residuals", col=7)
```

Residuals in months with lots of school holidays (March, summer,
December) are consistantly high. Let's create a dummy variable
called "holidays" that tells whether a particular passenger record
is for a month that has lots of holidays.
```{r}
YX$holidays <- airline$Month[t] %in% c(3,6,7,8,12)
```

Fit a new lm that adds this holiday variable on to what you had before, and then re-examine the residuals, including by month.
```{r}
## new fitted model and summary here

```

```{r, fig.align="center", fig.width=6, fig.height=4}
## plot of data + model fit here
```

```{r, fig.align="center", fig.width=6, fig.height=4}
par(mfrow=c(1,2))
## residuals plotted across time

## acf plot of the residuals

```

```{r, fig.align="center", fig.width=6, fig.height=4}
## boxplot of residuals by month

```


### Model Comparison

Now you have 2 nested models. Because they are nested, we can compare them in two ways. First I do a partial F-test. The idea behind the F-test is that it looks at the difference in the $R^2$ value between the two models and determines whether or not this difference is large enough to warrent the additional covariates. If we use the "anova" function in R and provide both fittend model objects as arguments, it will automatically perform a partial F-test. Note: you will need to replace mod1 and mod2 with the names of your model objects.

```{r}
## partial F test: anova(mod1, mod2)
```

___Q5: Based on these what model would you choose and why?___  


<br>

We can also compare the models via BIC (the Bayesian Information Criterion) and the approximate relative model probabilities based on the BICs. Note that we can use BIC/model probabilities to compare between models even if the models are not nested, as long as the response being modeled is the same in each case. Note: you will need to replace mod1 and mod2 with the names of your model objects.
```{r}
n<-length(YX$logY)-1
##bics<-c(mod1=extractAIC(mod1, k=log(n))[2],
##        mod2=extractAIC(mod2, k=log(n))[2])

##ebics<-exp(-0.5*(bics-min(bics)))

##probs<-ebics/sum(ebics)


##rbind(round(bics, 5), round(probs, 5))
```

___Q6: Which model is the best via BIC? Does this jive with what the partial F-test told you? What is the $R^2$ for your best model. Based on this model selection, $R^2$ and what you saw in the residuals for this model, do you feel satisfied that your model is capturing the patterns in the data? Would you want to go back and fit anything else?___


<br>
<br>


