{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< | [Main Contents](https://vectorbite.github.io/VBiTraining2/) | >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Climate Data to VecDyn \n",
    "##### Author: Deraj Wilson-Aggarwal, Imperial College London<br> deraj.wilson-aggarwal18@imperial.ac.uk <br> June 2019\n",
    "\n",
    "There are a plethora of different ways to download climate data. When downloading climate data, it is important to choose a database that has the best coverage possible for your given task as they will differ in their spatial and temporal resolution. \n",
    "\n",
    "Three main ways to consider are:\n",
    "\n",
    "- <b> Using pre-built Application Program Interfaces (APIs) such as RNOAA and RNCEP </b>\n",
    "    \n",
    "    APIs such as RNOAA and RNCEP are built packages for software such as R and Python that allows a user to extract climate data with ease. These packages call to servers to download the desired data. This method is great for smaller datasets, being reliable, convenient and efficient.\n",
    "    Although they are convenient, APIs may be difficult to use without steep learning curves. Documentation may not always be widely available. For larger datasets, connecting to servers drastically increases the computational time required to download the relevant data so it is not recommended. <br><br>\n",
    "    \n",
    "- <b> Downloading open source data files and extracting required data from them </b>\n",
    "\n",
    "    Downloading and extracting data manually from existing datasets generally requires a large amount of space and can seem clunky. However, when working on large datasets with a medium temporal resolution, they can be extremely efficient. The NetCDF file type is made specifically for climate data and are well suited for their purpose. For more info, please see https://www.unidata.ucar.edu/software/netcdf/docs/netcdf_introduction.html \n",
    "    <br><br>The main drawback of this method, is that it relies on external databases, as before, whilst also providing mid to low level temporal resolution. <br><br>\n",
    "\n",
    "- <b> Obtaining raster files from satellite data and extracting data using High Performance Computing (HPC) </b>\n",
    "\n",
    "    This is a method employed in GIS data analysis. Raster files can be huge, however have the ability to provide high temporal resolution in scales up to 5m. This makes them extremely powerful tools. Generally, the size of these files and the amount of datapoints requried, means that HPC methods are the most efficient ways of extracting the relevant data. <br><br>\n",
    "    \n",
    "    \n",
    "This notebook will demonstrate how to map climate data to VecDyn data using open source NetCDF NOAA files. The example used is a subset of VecDyn and spans only 2 years, so will be relativlely small in size. I recommend to run any larger datasets using R or RStudio. Feedback on this notebook and its methodology are encouraged, so please do share alternative methods and suggestions with me. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, let's download the data we need.  \n",
    "\n",
    "You may download publicly available NOAA datasets following this link: ftp://ftp.cdc.noaa.gov/Datasets\n",
    "My suggested datasets, and the ones I have chose to use are are:\n",
    "- ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_temp/ for daily maximum temperature data (degrees celsius) \n",
    "- ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_precip/ for daily Precipitation \n",
    "\n",
    "These datasets are available as yearly files, with a spatial resolution of 0.5 degrees latitude and longitude (intervals are at .25 and .75). \n",
    "\n",
    "Unfortunately, the NOAA file co-ordinates are mapped slightly differently to that of VecDyn, so we need to match these together. The Climate Data Operators (CDO) can allow us to do this, so you will need to download the CDO application (https://code.mpimet.mpg.de/projects/cdo) and run the following code on the command line (from terminal):\n",
    "\n",
    "``` cdo -z zip sellonlatbox,-180,180,-90,90 INPUT_FILE.nc OUTPUT_FILE.nc  ``` \n",
    "\n",
    "The following code will run through R to download the climate data you will require for this example. \n",
    "\n",
    "   <b> Please ensure you have the CDO application downloaded so that you can run it from the command line. <br><br> These files may take 5 minutes to download </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This download will take a couple of minutes as the files are about 60mb each! \n",
    "\n",
    "Years_to_download = c(2015,2016)\n",
    "\n",
    "for (year in Years_to_download){\n",
    "    \n",
    "    # download the files that we require\n",
    "    download.file(paste(\"ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_precip/precip.\",year,\".nc\", sep=\"\"), paste(\"../data/precip.\",year,\".nc\",sep=\"\"))\n",
    "    download.file(paste(\"ftp://ftp.cdc.noaa.gov/Datasets/cpc_global_temp/tmax.\",year,\".nc\", sep=\"\"), paste(\"../data/tmax.\",year,\".nc\",sep=\"\"))\n",
    "    \n",
    "    # Use R to go to the terminal and call the cdo remapping \n",
    "    system(paste(\"cdo -z zip sellonlatbox,-180,180,-90,90 ../data/precip.\", year, \".nc ../data/precip.\", year, \".new.nc\", sep=\"\"))\n",
    "    system(paste(\"cdo -z zip sellonlatbox,-180,180,-90,90 ../data/tmax.\", year, \".nc ../data/tmax.\", year, \".new.nc\", sep=\"\"))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load in our VecDyn data that we need climate data to map to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(ncdf4)\n",
    "#install.packages(chron)\n",
    "#install.packages(tidyr)\n",
    "\n",
    "library(ncdf4)\n",
    "library(chron)\n",
    "library(tidyr)\n",
    "\n",
    "\n",
    "# read in the data file without climate data \n",
    "Data_no_climate <- read.csv(\"../data/example_import_data.csv\")\n",
    "\n",
    "Data_climate <- Data_no_climate\n",
    "\n",
    "# To keep things organised, let's rename the columns \n",
    "names(Data_climate)[35] <- \"Initial_latitude\"\n",
    "names(Data_climate)[36] <- \"Initial_longitude\"\n",
    "names(Data_climate)[28] <- \"Collection.date.range\"\n",
    "\n",
    "##\n",
    "## Please note that the indexes above may be a column out \n",
    "## due to including row names column when writing the csv \n",
    "## \n",
    "## If this doesnt work, please try the following code instead and re run:\n",
    "#\n",
    "#names(Data_climate)[34] <- \"Initial_latitude\"\n",
    "#names(Data_climate)[35] <- \"Initial_longitude\"\n",
    "#names(Data_climate)[27] <- \"Collection.date.range\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write a couple of functions to incorporate into our loop we use to populate the climate data columns. Some of the co-ordinates used are near to the coast - therefore it is possible that the we have ```NA``` inputs where the co-ordinate references the sea. \n",
    "\n",
    "Within the loop, we will include a number of \"safety\" statements in the form of if/else statements. Therefore, we will be able to catch any ```NA```s in our data frame and find the closest values we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to round our co-ordinates to the nearest .25 value \n",
    "rounding <- function(value){\n",
    "  # round to the nearest integer\n",
    "  x <- round(value, digits = 0)\n",
    "  # if rounded up (x>value) reduce by 0.25\n",
    "  # else add 0.25 \n",
    "  new = ifelse(x>value, x-0.25, x+0.25)\n",
    "  return(new)\n",
    "}\n",
    "\n",
    "# A function to round to the other nearest 0.25 \n",
    "rounding_opposite <- function(value){\n",
    "  # round to the nearest integer\n",
    "  x <- round(value, digits = 0)\n",
    "  # if rounded up (x>value) reduce by 0.25\n",
    "  # else add 0.25 \n",
    "  new = ifelse(x>value, x+0.25, x-0.25)\n",
    "  return(new)\n",
    "}\n",
    "\n",
    "# A function to force rounding up if required \n",
    "force_round_up <- function(value){\n",
    "  # round UP the nearest integer \n",
    "  # minus 0.25 to back towards value \n",
    "  x <- ceiling(value) - 0.25\n",
    "  return(x)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined some functions to round, we can apply them to our current co-ordinate columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the latitude and longitude using our previous functions\n",
    "# The new co-ordinates are saved as new columns \n",
    "Data_climate$latitude <- rounding(as.numeric(Data_climate$Initial_latitude))\n",
    "Data_climate$longitude <- rounding(as.numeric(Data_climate$Initial_longitude))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to ensure that the columns we use are correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date \n",
    "Data_climate$Collection.date.range <- as.Date(Data_climate$Collection.date.range)\n",
    "\n",
    "# create a column for the Julian date for later slicing  \n",
    "Data_climate$Julian <- as.numeric(format(Data_climate$Collection.date.range, \"%j\"))\n",
    "\n",
    "# extract year fro mthe date \n",
    "Data_climate$Year <- as.numeric(format(Data_climate$Collection.date.range, \"%Y\"))\n",
    "\n",
    "# order by date and reset indexes \n",
    "Data_climate <- Data_climate[order(Data_climate$Collection.date.range, decreasing = FALSE),]\n",
    "rownames(Data_climate) <- NULL\n",
    "\n",
    "# Create a column of NAs to populate \n",
    "Data_climate$Precipitation <- NA\n",
    "\n",
    "# Create a column of NAs to populate \n",
    "Data_climate$Max.Temp <- NA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is where it might get a little messy and difficult to follow - so bear with me and read through the comments in the loop below one by one. The code is (currently) rather verbose, and is in the process of being optimised. All suggestions are very welcome!\n",
    "\n",
    "The process, in words, is as follows: \n",
    "\n",
    "- For each unique year, find the row indexes that apply to that year\n",
    "- Open the NetCDF files for that year relating to Precipitation and Maximum Temperature \n",
    "- Loop through each row index and extract the lat/long\n",
    "- Extract the precipitation and temperature value for the lat/long\n",
    "- If there is an ```NA``` then try again with different lat/long combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i <- 0\n",
    "Year1 <- 0\n",
    "\n",
    "# for each unique year in the dataframe \n",
    "for (Year in unique(Data_climate$Year)){\n",
    "  print(paste(\"Adding climate data for the year \", Year, sep=\"\"))\n",
    "  # Draw out the index values in the dataset that correspond to that year \n",
    "  Year_Index <- which(Data_climate[,\"Year\"] == Year)\n",
    "  \n",
    "  ###############################################\n",
    "  # Extract daily precipitation \n",
    "  ###############################################\n",
    "  \n",
    "  print(\"Opening NetCDF precipitation file\")\n",
    "  # Define the netCDF file for that year stored in the data directory (please ask for this data as it is very large)\n",
    "  ncpath_prp <- \"../data/\"\n",
    "  ncfname_prp <- paste(ncpath_prp, \"precip.\", Year, \".new\", \".nc\", sep=\"\")\n",
    "  dname_prp <- \"precip\" \n",
    "  \n",
    "  # open the netCDF file\n",
    "  ncin_prp <- nc_open(ncfname_prp)\n",
    "  \n",
    "  # get longitude and latitude for the factor \n",
    "  lon_prp <- ncvar_get(ncin_prp,\"lon\")\n",
    "  lat_prp <- ncvar_get(ncin_prp,\"lat\")\n",
    "  \n",
    "  # get the time for that factor \n",
    "  time <- ncvar_get(ncin_prp,\"time\")\n",
    "  tunits <- ncatt_get(ncin_prp,\"time\",\"units\")\n",
    "  nt <- dim(time)\n",
    "  \n",
    "  # Create an array for the precipitation values \n",
    "  prp_array <- ncvar_get(ncin_prp,dname_prp)\n",
    "  \n",
    "  # other factors that may be required/useful\n",
    "  dlname <- ncatt_get(ncin_prp,dname_prp,\"long_name\")\n",
    "  dunits <- ncatt_get(ncin_prp,dname_prp,\"units\")\n",
    "  fillvalue <- ncatt_get(ncin_prp,dname_prp,\"_FillValue\")\n",
    "  \n",
    "  # replace netCDF fill values with NA's\n",
    "  prp_array[prp_array==fillvalue$value] <- NA\n",
    "  \n",
    "  # create a matrix for the lon/lats of that file \n",
    "  lonlat_prp <- as.matrix(expand.grid(lon_prp,lat_prp))\n",
    "  \n",
    "  ###############################################\n",
    "  # Extract max daily temperature\n",
    "  ###############################################\n",
    "  \n",
    "  print(\"Opening NetCDF temperature file\")\n",
    "  ncpath_tmp <- \"../data/\"\n",
    "  ncfname_tmp <- paste(ncpath_tmp, \"tmax.\", Year, \".new\", \".nc\", sep=\"\")\n",
    "  dname <- \"tmax\" \n",
    "  \n",
    "  \n",
    "  # open the netCDF file\n",
    "  ncin_tmp <- nc_open(ncfname_tmp)\n",
    "  \n",
    "  # get longitude and latitude for factor \n",
    "  lon_tmp <- ncvar_get(ncin_tmp,\"lon\")\n",
    "  lat_tmp <- ncvar_get(ncin_tmp,\"lat\")\n",
    "  \n",
    "  # extract time\n",
    "  time <- ncvar_get(ncin_tmp,\"time\")\n",
    "  tunits <- ncatt_get(ncin_tmp,\"time\",\"units\")\n",
    "  nt <- dim(time)\n",
    "  \n",
    "  # get temperature \n",
    "  tmp_array <- ncvar_get(ncin_tmp,dname)\n",
    "  \n",
    "  dlname <- ncatt_get(ncin_tmp,dname,\"long_name\")\n",
    "  dunits <- ncatt_get(ncin_tmp,dname,\"units\")\n",
    "  fillvalue <- ncatt_get(ncin_tmp,dname,\"_FillValue\")\n",
    "  \n",
    "  # replace netCDF fill values with NA's\n",
    "  tmp_array[tmp_array==fillvalue$value] <- NA\n",
    "  \n",
    "  # define a matric with temperature lon/lats\n",
    "  lonlat_tmp <- as.matrix(expand.grid(lon_tmp,lat_tmp))\n",
    "  print(paste(\"Running for the year \", Year,\". There are \", length(Year_Index), \" rows, from \", min(Year_Index), \"to\", max(Year_Index)))\n",
    "  \n",
    "  # loop to extract climate data and save\n",
    "  for (row in Year_Index){\n",
    "    # define the Julian day for that row \n",
    "    j <- Data_climate[row, \"Julian\"]\n",
    "    # define the lon/lats of the row \n",
    "    lon_n <- Data_climate[row, \"longitude\"]\n",
    "    lat_n <- Data_climate[row, \"latitude\"]\n",
    "    \n",
    "    # If the julian day differs, or the year differs from the previous row\n",
    "    if (j!=i | Year != Year1){\n",
    "      # redefine i as j \n",
    "      i <- j\n",
    "      # Slice the temp data into the day required\n",
    "      tmp_slice <- tmp_array[,,i]\n",
    "      # store as a vector\n",
    "      tmp_vec <- as.vector(tmp_slice)\n",
    "      # bind as a dataframe with the lon/lats of the factor \n",
    "      tmp_df01 <- data.frame(cbind(lonlat_tmp,tmp_vec))\n",
    "      # add the temp value for the rows long/lats on tht day  \n",
    "      Data_climate[row, \"Max.Temp\"] <- with(tmp_df01, tmp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "      \n",
    "      # Same again but for precipitation \n",
    "      prp_slice <- prp_array[,,i]\n",
    "      prp_vec <- as.vector(prp_slice)\n",
    "      prp_df01 <- data.frame(cbind(lonlat_prp,prp_vec))\n",
    "      Data_climate[row, \"Precipitation\"] <- with(prp_df01, prp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "      \n",
    "    }\n",
    "    # otherwise, if Julian day does not differ NOR does the year \n",
    "    else{\n",
    "      # Go straight in and extract from the existing slice for that lon/lat\n",
    "      Data_climate[row, \"Max.Temp\"] <- with(tmp_df01, tmp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "      Data_climate[row, \"Precipitation\"] <- with(prp_df01, prp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "      \n",
    "    }\n",
    "    # redefine the previous year once the years index has been completed\n",
    "    \n",
    "    if (is.na(Data_climate[row, \"Max.Temp\"])){\n",
    "      # Try a different latitude\n",
    "      Data_climate[row, \"latitude\"] <- rounding(Data_climate[row, \"Initial_latitude\"])\n",
    "      Data_climate[row, \"longitude\"] <- rounding_opposite(Data_climate[row, \"Initial_longitude\"])\n",
    "      \n",
    "      # define the lon/lats of the row\n",
    "      lon_n <- Data_climate[row, \"longitude\"]\n",
    "      lat_n <- Data_climate[row, \"latitude\"]\n",
    "      #print(paste(\"Attempting again for row \", row, \"at lon/lat \", lon_n, \",\",lat_n))\n",
    "      \n",
    "      \n",
    "      # add the temp value for the rows long/lats on tht day\n",
    "      Data_climate[row, \"Max.Temp\"] <- with(tmp_df01, tmp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "      \n",
    "      \n",
    "      if (is.na(Data_climate[row,\"Max.Temp\"])){\n",
    "        # try a different longitude\n",
    "        Data_climate[row, \"latitude\"] <- rounding_opposite(Data_climate[row, \"Initial_latitude\"])\n",
    "        Data_climate[row, \"longitude\"] <- rounding(Data_climate[row, \"Initial_longitude\"])\n",
    "        \n",
    "        # define the lon/lats of the row\n",
    "        lon_n <- Data_climate[row, \"longitude\"]\n",
    "        lat_n <- Data_climate[row, \"latitude\"]\n",
    "        #print(paste(\"Attempting second time for row \", row, \"at lon/lat \", lon_n, \",\", lat_n))\n",
    "        \n",
    "        Data_climate[row, \"Max.Temp\"] <- with(tmp_df01, tmp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "        \n",
    "        if (is.na(Data_climate[row,\"Max.Temp\"])){\n",
    "          # try a different longitude\n",
    "          Data_climate[row, \"latitude\"] <- rounding_opposite(Data_climate[row, \"Initial_latitude\"])\n",
    "          Data_climate[row, \"longitude\"] <- rounding_opposite(Data_climate[row, \"Initial_longitude\"])\n",
    "          \n",
    "          # define the lon/lats of the row\n",
    "          lon_n <- Data_climate[row, \"longitude\"]\n",
    "          lat_n <- Data_climate[row, \"latitude\"]\n",
    "          print(paste(\"Attempting third time for row \", row, \"at lon/lat \",lon_n,\",\",lat_n))\n",
    "          \n",
    "          Data_climate[row, \"Max.Temp\"] <- with(tmp_df01, tmp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "          \n",
    "          if (is.na(Data_climate[row,\"Max.Temp\"])){\n",
    "            # Try a different latitude\n",
    "            Data_climate[row, \"latitude\"] <- rounding(Data_climate[row, \"Initial_latitude\"])\n",
    "            Data_climate[row, \"longitude\"] <- force_round_up(Data_climate[row, \"Initial_longitude\"])\n",
    "            \n",
    "            # define the lon/lats of the row\n",
    "            lon_n <- Data_climate[row, \"longitude\"]\n",
    "            lat_n <- Data_climate[row, \"latitude\"]\n",
    "            #print(paste(\"Attempting again for row \", row, \"at lon/lat \", lon_n, \",\",lat_n))\n",
    "            \n",
    "            \n",
    "            # add the temp value for the rows long/lats on tht day\n",
    "            Data_climate[row, \"Max.Temp\"] <- with(tmp_df01, tmp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "            \n",
    "            if (is.na(Data_climate[row, \"Max.Temp\"])){\n",
    "              \n",
    "              Data_climate[row, \"latitude\"] <- force_round_up(Data_climate[row, \"Initial_latitude\"])\n",
    "              Data_climate[row, \"longitude\"] <- rounding(Data_climate[row, \"Initial_longitude\"])\n",
    "              \n",
    "              # define the lon/lats of the row\n",
    "              lon_n <- Data_climate[row, \"longitude\"]\n",
    "              lat_n <- Data_climate[row, \"latitude\"]\n",
    "              #print(paste(\"Attempting second time for row \", row, \"at lon/lat \", lon_n, \",\",lat_n))\n",
    "              \n",
    "              \n",
    "              # add the temp value for the rows long/lats on tht day\n",
    "              Data_climate[row, \"Max.Temp\"] <- with(tmp_df01, tmp_vec[Var2 == lat_n & Var1 == lon_n])\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    Year1 == Year\n",
    "  } \n",
    "  \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to check the values that we've obtained are as we expect using ```summary()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Data_climate$Precipitation)\n",
    "summary(Data_climate$Max.Temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save our output to a new ```.csv``` file, ready to import and analyse:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write.csv(Data_climate, \"../data/example_import_data_with_climate.csv\", row.names=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
